{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this file has APIS for 4 different purpose, these can be used to automate our tasks\n",
    "#within each API there are more functions that can be accessed using apporopriate routing provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_15116\\469756692.py:1: DtypeWarning: Columns (1,4,7,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data= pd.read_csv('songs.csv')\n"
     ]
    }
   ],
   "source": [
    "data= pd.read_csv('dirty_songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:6500\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "#removing dirty data API\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return 'hello we will be removing dirty data through this'\n",
    "# Route to detect and remove potential data entry errors\n",
    "@app.route('/detect_errors')\n",
    "def detect_and_remove_errors(file_path):\n",
    "    # Initialize a list to store potential errors\n",
    "    errors = []\n",
    "\n",
    "    # Check for negative values in numeric columns\n",
    "    numeric_columns = data.select_dtypes(include=['float', 'int']).columns\n",
    "    for column_name in numeric_columns:\n",
    "        negative_values = data[column_name] < 0\n",
    "        if negative_values.any():\n",
    "            error = f\"Column '{column_name}' has {negative_values.sum()} negative value(s)\"\n",
    "            errors.append(error)\n",
    "\n",
    "    # Check for text values that are too long\n",
    "    text_columns = data.select_dtypes(include=['object']).columns\n",
    "    for column_name in text_columns:\n",
    "        if data[column_name].str.len().max() > 100:\n",
    "            error = f\"Column '{column_name}' has text values that are too long\"\n",
    "            errors.append(error)\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    data.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Remove columns with too many repeated values\n",
    "    for column_name in data.columns:\n",
    "        if data[column_name].nunique() < len(data) / 10:\n",
    "            data.drop(columns=[column_name], inplace=True)\n",
    "\n",
    "    # Return the potential errors and cleaned data as a JSON response\n",
    "    if errors:\n",
    "        return jsonify({'errors': errors, 'data': data.to_dict()})\n",
    "    else:\n",
    "        return jsonify({'message': 'No potential errors detected', 'data': data.to_dict()})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=6500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API for analysing missing values\n",
    "from flask import Flask, jsonify\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Read the data from a CSV file\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return 'hello we will be analysis missing values'\n",
    "# Route to perform missing value analysis on a dataset\n",
    "@app.route('/missing_analysis')\n",
    "def missing_value_analysis():\n",
    "    # Calculate the percentage of missing values for each column\n",
    "    missing_percentage = round(data.isnull().sum() / len(data) * 100, 2)\n",
    "\n",
    "    # Identify columns with missing values\n",
    "    missing_columns = data.columns[data.isnull().any()].tolist()\n",
    "\n",
    "    # Create a dictionary to store the missing value analysis results\n",
    "    results = {\n",
    "        'missing_percentage': missing_percentage.to_dict(),\n",
    "        'missing_columns': missing_columns,\n",
    "    }\n",
    "\n",
    "    # If any columns have missing values, also perform imputation\n",
    "    if missing_columns:\n",
    "        # Identify the data types of the columns with missing values\n",
    "        missing_column_types = data[missing_columns].dtypes\n",
    "\n",
    "        # Impute missing values for numeric columns with the column mean\n",
    "        numeric_columns = missing_column_types[missing_column_types != 'object'].index.tolist()\n",
    "        if numeric_columns:\n",
    "            imputed_data = data.copy()\n",
    "            for column_name in numeric_columns:\n",
    "                mean = imputed_data[column_name].mean()\n",
    "                imputed_data[column_name].fillna(mean, inplace=True)\n",
    "            results['imputed_data'] = imputed_data.head(10).to_dict(orient='records')\n",
    "\n",
    "        # Impute missing values for text columns with the mode\n",
    "        text_columns = missing_column_types[missing_column_types == 'object'].index.tolist()\n",
    "        if text_columns:\n",
    "            imputed_data = data.copy()\n",
    "            for column_name in text_columns:\n",
    "                mode = imputed_data[column_name].mode()[0]\n",
    "                imputed_data[column_name].fillna(mode, inplace=True)\n",
    "            results['imputed_data'] = imputed_data.head(10).to_dict(orient='records')\n",
    "\n",
    "    # Return the missing value analysis results as a JSON response\n",
    "    return jsonify(results)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=7000) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APIS for statistical tests:\n",
    "# Import libraries\n",
    "from flask import Flask, jsonify, request\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create a Flask app object\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \"Hello you can add your staistical tests routes to generate specific tests\"\n",
    "\n",
    "# Define routes using the @app decorator\n",
    "\n",
    "@app.route('/boxplot')\n",
    "def boxplot():\n",
    "    column_name = 'acousticness' #this can be modified according to your choice\n",
    "    column_data = data[column_name]\n",
    "\n",
    "    # Ignore missing values\n",
    "    column_data = column_data[~column_data.isna()]\n",
    "\n",
    "    # Generate the boxplot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(column_data)\n",
    "    ax.set_title(f\"Boxplot of {column_name}\")\n",
    "    ax.set_xlabel(column_name)\n",
    "    ax.set_ylabel(\"Value\")\n",
    "\n",
    "    # Save the plot to a file and return the file path\n",
    "    plot_path = 'boxplot.png' #it will be saved in your downloads, and can simply be seen when you quit from API running\n",
    "    plt.savefig(plot_path)\n",
    "\n",
    "    return jsonify({'plot_path': plot_path})\n",
    "\n",
    "\n",
    "@app.route('/histogram')\n",
    "def histogram():\n",
    "    column_name = 'acousticness'\n",
    "    column_data = data[column_name]\n",
    "\n",
    "    # Generate the histogram\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(column_data.dropna(), bins=10)\n",
    "    ax.set_title(f\"Histogram of {column_name}\")\n",
    "    ax.set_xlabel(column_name)\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "    # Save the plot to a file and return the file path\n",
    "    plot_path = 'histogram.png'\n",
    "    plt.savefig(plot_path)\n",
    "\n",
    "    return jsonify({'plot_path': plot_path})\n",
    "\n",
    "@app.route('/correlation')\n",
    "def correlation():\n",
    "    column_name = 'acousticness'\n",
    "    \n",
    "    # Get the correlation matrix for the specified column\n",
    "    corr = data.corr()[[column_name]]\n",
    "    \n",
    "    # Generate the heatmap\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, ax=ax)\n",
    "    ax.set_title(f\"Correlation Matrix for {column_name}\")\n",
    "    \n",
    "    # Save the plot to a file and return the file path\n",
    "    plot_path = 'correlation.png'\n",
    "    plt.savefig(plot_path)\n",
    "\n",
    "    return jsonify({'plot_path': plot_path})\n",
    "\n",
    "@app.route('/regression')\n",
    "def regression():\n",
    "    x_col = 'energy'\n",
    "    y_col = 'tempo'\n",
    "\n",
    "    # Generate the regression plot\n",
    "    sns.regplot(data=data, x=x_col, y=y_col, dropna=False)\n",
    "\n",
    "    # Save the plot to a file and return the file path\n",
    "    plot_path = 'regplot.png'\n",
    "    plt.savefig(plot_path)\n",
    "\n",
    "    return jsonify({'plot_path': plot_path})\n",
    "\n",
    "\n",
    "@app.route('/chisquare')\n",
    "def chisquare():\n",
    "    column1_name = 'energy'\n",
    "    column2_name = 'danceability'\n",
    "    contingency_table = pd.crosstab(data[column1_name], data[column2_name])\n",
    "    \n",
    "    # Perform chi-square test\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "    # Return the results as JSON response\n",
    "    result = {\n",
    "        'test': 'Chi-Square Test',\n",
    "        'variables': f'{column1_name} vs {column2_name}',\n",
    "        'chi2_statistic': chi2,\n",
    "        'p_value': p_value,\n",
    "        'degrees_of_freedom': dof,\n",
    "        'expected_values': expected.tolist()\n",
    "    }\n",
    "    return jsonify(result)\n",
    "\n",
    "@app.route('/anova')\n",
    "def anova():\n",
    "    # Read the data from the CSV file\n",
    "    data = pd.read_csv('path/to/file.csv')\n",
    "\n",
    "    # Extract the data for the two columns and ignore missing values\n",
    "    column1_name = 'energy'\n",
    "    column2_name = 'tempo'\n",
    "    column1_data = data[column1_name][~data[column1_name].isnull()]\n",
    "    column2_data = data[column2_name][~data[column2_name].isnull()]\n",
    "\n",
    "    # Perform the ANOVA test\n",
    "    f_value, p_value = stats.f_oneway(column1_data, column2_data)\n",
    "\n",
    "    # Print the results of the test\n",
    "    if p_value < 0.05:\n",
    "        result = \"The difference between the means of the two columns is statistically significant (p-value < 0.05)\"\n",
    "    else:\n",
    "        result = \"There is no statistically significant difference between the means of the two columns (p-value >= 0.05)\"\n",
    "\n",
    "    # Return the results as a JSON response\n",
    "    return jsonify({'result': result})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API to directly identify outliers and remove the ones that are over an extreme range\n",
    "from flask import Flask, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Route to detect outliers in a given column\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \"hello, we will be detecting outliers and removing extreme ones here\"\n",
    "\n",
    "@app.route('/detect_outliers')\n",
    "def detect_outliers():\n",
    "    column_name = 'acousticness'\n",
    "    iqr_factor = 1.5\n",
    "    column_data = raw[column_name]\n",
    "\n",
    "    # Calculate the IQR of the column data\n",
    "    q1 = column_data.quantile(0.25)\n",
    "    q3 = column_data.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Define the upper and lower bounds for outlier detection\n",
    "    # Define the upper and lower bounds for outlier detection\n",
    "    upper_bound = q3 + iqr_factor * (3 * iqr)\n",
    "    lower_bound = q1 - iqr_factor * (3 * iqr)\n",
    "\n",
    "    # Find the outliers in the column data\n",
    "    outliers = column_data[(column_data > upper_bound) | (column_data < lower_bound)]\n",
    "\n",
    "    # Remove the outliers that are even 1.5 times outside the range\n",
    "    remove_outliers = outliers[(outliers > upper_bound * 1.5) | (outliers < lower_bound * 1.5)]\n",
    "    column_data_cleaned = column_data[~column_data.isin(remove_outliers)]\n",
    "\n",
    "    # Convert the outliers to a list and return as JSON\n",
    "    outliers_list = remove_outliers.tolist()\n",
    "    return jsonify({'outliers': outliers_list, 'column_data_cleaned': column_data_cleaned.tolist()})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
